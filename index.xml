<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Qian Yin&#39;s Homepage</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Qian Yin&#39;s Homepage</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>http://localhost:1313/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/</guid>
      <description>&lt;section id=&#34;publications&#34;&gt;&#xD;&#xA;&lt;div class=&#34;pub-header&#34;&gt;&#xD;&#xA;  &lt;span class=&#34;title&#34;&gt;Publications&lt;/span&gt;&#xD;&#xA;  &lt;span class=&#34;sub&#34;&gt;( &lt;a href=&#34;https://scholar.google.com/citations?user=Q4e8ZvAAAAAJ&amp;hl=zh-CN&#34; target=&#34;_blank&#34;&gt;Google Scholar Profile&lt;/a&gt; )&lt;/span&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;h1 id=&#34;journal&#34;&gt;Journal Publications&lt;/h1&gt;&#xD;&#xA;&lt;ol id=&#34;journal-list&#34;&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Prediction Enhancement for Point Cloud Attribute Compression using Smoothing Filter&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        &lt;strong&gt;Qian Yin&lt;/strong&gt;, Ruoke Yan, Xinfeng Zhang and Siwei Ma&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;journal small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)&lt;/em&gt;&#xD;&#xA;        &#xD;&#xA;        &#xD;&#xA;        , &lt;em&gt;pp. 1-1&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2025&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/TCSVT.2025.3571114&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;abs1&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bib1&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;abs1&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; In recent years, 3D point cloud compression (PCC) has emerged as a prominent research area, attracting widespread attention from both academia and industry. As one of the PCC standards released by the moving picture expert group (MPEG), the geometry-based PCC (G-PCC) adopts two attribute lossy coding schemes, namely the prediction-based Lifting Transform and the region adaptive hierarchical transform (RAHT). Based on statistical analysis, it can be observed that the increase in predictive distance gradually weakens the attribute correlation between points, resulting in larger prediction errors. To address this issue, we propose a prediction enhancement method by using the smoothing filter to improve the attribute coding efficiency, which is both integrated into the Lifting Transform and RAHT. For the former, the neighbor point smoothing method based on the prediction order is proposed via a weighted average strategy. The proposed smoothing is only applied to points in the lower level of details (LoDs) by adjusting the distance-based predicted attribute values. For the latter, we design a neighbor node smoothing method after the inter depth up-samping (IDUS) prediction, where the sub-nodes in the same unit node are filtered for lower levels. Experimental results have demonstrated that compared with two latest MPEG G-PCC reference software TMC13-v23.0 and GeSTM-v3.0, our proposed enhanced prediction method exhibits superior Bj√∏ntegaard delta bit rate (BDBR) gains with small increase in time complexity.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bib1&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@ARTICLE{11006727,&#xA;  author={Yin, Qian and Yan, Ruoke and Zhang, Xinfeng and Ma, Siwei},&#xA;  journal={IEEE Transactions on Circuits and Systems for Video Technology}, &#xA;  title={Prediction Enhancement for Point Cloud Attribute Compression using Smoothing Filter}, &#xA;  year={2025},&#xA;  volume={},&#xA;  number={},&#xA;  pages={1-1},&#xA;  doi={10.1109/TCSVT.2025.3571114}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Joint Structure-Texture Scan-Order for Point Cloud Attribute Compression Using Affine Transformation&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        &lt;strong&gt;Qian Yin&lt;/strong&gt;, Xinfeng Zhang, Ruoke Yan, Yuhuai Zhang, Shanshe Wang and Siwei Ma&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;journal small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)&lt;/em&gt;&#xD;&#xA;        &#xD;&#xA;        &#xD;&#xA;        &#xD;&#xA;        , &lt;em&gt;2025&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1145/3729232&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;abs2&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bib2&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;abs2&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; Existing geometry-based point cloud compression (PCC) frameworks are typically designed to code the geometric coordinates first, followed by compressing the attributes (e.g., colors, reflectances) according to the order derived from geometric structures, such as the morton codes. Although geometry-based reordering methods can eliminate the redundancy of attributes, the errors caused by dramatic variations of attributes in the non-smooth areas potentially limit the efficiency of the point cloud attribute coding. To tackle this challenge, a novel joint structure-texture scan-order and coding scheme is proposed, which aims to explore a better attribute coding order from the viewpoint of improving the geometry-attribute consistency. Specifically, we formulate the attribute reordering problem as a geometry-attribute alignment task, and utilize the affine-transform model to find the optimal correspondence between geometry and attribute information by minimizing attributed prediction residuals. Then, the morton codes based point cloud reordering is conducted on the transformed point cloud. Experimental results conducted on the various benchmark datasets have illustrated that our proposed method outperforms the MPEG standard G-PCC with gains of up to 2%, 9% and 7% in luma, chroma and reflectance, respectively.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bib2&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@article{10.1145/3729232,&#xA;author = {Yin, Qian and Zhang, Xinfeng and Yan, Ruoke and Zhang, Yuhuai and Wang, Shanshe and Ma, Siwei},&#xA;title = {Joint Structure-Texture Scan-Order for Point Cloud Attribute Compression Using Affine Transformation},&#xA;journal = {ACM Trans. Multimedia Comput. Commun. Appl.},&#xA;year = {2025},&#xA;note = {Just Accepted},&#xA;doi = {10.1145/3729232},&#xA;url = {https://doi.org/10.1145/3729232}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Content-Adaptive Rate Control Method for User-Generated Content Videos&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Longtao Feng, &lt;strong&gt;Qian Yin&lt;/strong&gt; and Siwei Ma&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;journal small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;vol. 35&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;no. 3&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 2806-2819&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2025&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/TCSVT.2024.3486095&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;abs3&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bib3&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;abs3&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; In recent years, user-generated content (UGC) videos have become the mainstream of internet videos, which are characterized by their rich content, complicated temporal changes and multiple distortions. However, existing rate control (RC) methods do not consider the above unique characteristics, leading to severe bit-rate errors and coding performance degradation. To address these issues, we propose a content-adaptive RC method for UGC videos, where accurate RC coding parameters are derived by our proposed rate-distortion (RD) model derivations for different types of pictures and a novel bit allocation refinement module. Specifically, the RD models of intra pictures are derived by established SVR-based predictors using some features designed for diverse content, such as texture complexity and regularity. Considering the complex temporal variation, single-reference inter pictures are firstly classified into three categories (i.e., low, regular and high correlation) by a SVM-based classifier using correlation-based features. Training data of the classifier are labeled by introducing a series of classification metrics. Then, RD model is derived by established predictors accordingly for each type of inter pictures. In addition, the RD model of multiple-reference inter pictures is derived by using a updated RD model selection based on content similarity. Based on derived RD models, allocated bits are refined to reduce bit waste. Experimental results show that compared with the default RC method in versatile video coding (VVC), our method can effectively save BD-Rate and reduce bit-rate errors for UGC videos. In particular, 1.99% BD-Rate saving and 0.18% bit-rate error reduction can be achieved under the random access (RA) configuration, and 0.45% BD-Rate saving under the low-delay B (LDB) configuration.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bib3&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@ARTICLE{10734399,&#xA;author={Feng, Longtao and Yin, Qian and Ma, Siwei},&#xA;journal={IEEE Transactions on Circuits and Systems for Video Technology}, &#xA;title={Content-Adaptive Rate Control Method for User-Generated Content Videos}, &#xA;year={2025},&#xA;volume={35},&#xA;number={3},&#xA;pages={2806-2819},&#xA;doi={10.1109/TCSVT.2024.3486095}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Pose-Driven Compression for Dynamic 3D Human via Human Prior Models&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Ruoke Yan, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Xinfeng Zhang, Qi Zhang, Gai Zhang and Siwei Ma&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;journal small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;vol. 46&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;no. 8&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 5820-5834&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2024&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2024.3368567&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;abs4&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bib4&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;abs4&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; To cost-effectively transmit high-quality dynamic 3D human images in immersive multimedia applications, efficient data compression is crucial. Unlike existing methods that focus on reducing signal-level reconstruction errors, we propose the first dynamic 3D human compression framework based on human priors. The layered coding architecture significantly enhances the perceptual quality while also supporting a variety of downstream tasks, including visual analysis and content editing. Specifically, a high-fidelity pose-driven Avatar is generated from the original frames as the basic structure layer to implicitly represent the human shape. Then, human movements between frames are parameterized via a commonly-used human prior model, i.e., the Skinned Multi-Person Linear Model (SMPL), to form the motion layer and drive the Avatar. Furthermore, the normals are also introduced as an enhancement layer to preserve fine-grained geometric details. Finally, the Avatar, SMPL parameters, and normal maps are efficiently compressed into layered semantic bitstreams. Extensive qualitative and quantitative experiments show that the proposed framework remarkably outperforms other state-of-the-art 3D codecs in terms of subjective quality with only a few bits.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bib4&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@ARTICLE{10443540,&#xA;author={Yan, Ruoke and Yin, Qian and Zhang, Xinfeng and Zhang, Qi and Zhang, Gai and Ma, Siwei},&#xA;journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, &#xA;title={Pose-Driven Compression for Dynamic 3D Human via Human Prior Models}, &#xA;year={2024},&#xA;volume={46},&#xA;number={8},&#xA;pages={5820-5834},&#xA;doi={10.1109/TPAMI.2024.3368567}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Lossy Point Cloud Attribute Compression with Subnode-Based Prediction&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        &lt;strong&gt;Qian Yin&lt;/strong&gt;, Xinfeng Zhang, Hongyue Huang, Shanshe Wang and Siwei Ma&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;journal small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;ZTE Communications&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;vol. 21&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;no. 4&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 29-37&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2023&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://zte.magtechjournal.com/CN/10.12142/ZTECOM.202304004&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;abs5&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bib5&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;abs5&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; Recent years have witnessed that 3D point cloud compression (PCC) has become a research hotspot both in academia and industry. Especially in industry, the Moving Picture Expert Group (MPEG) has actively initiated the development of PCC standards. One of the adopted frameworks called geometry-based PCC (G-PCC) follows the architecture of coding geometry first and then coding attributes, where the region adaptive hierarchical transform (RAHT) method is introduced for the lossy attribute compression. The upsampled transform domain prediction in RAHT does not sufficiently explore the attribute correlations between neighbor nodes and thus fails to further reduce the attribute redundancy between neighbor nodes. In this paper, we propose a subnode-based prediction method, where the spatial position relationship between neighbor nodes is fully considered and prediction precision is further promoted. We utilize some already-encoded neighbor nodes to facilitate the upsampled transform domain prediction in RAHT by means of a weighted average strategy. Experimental results have illustrated that our proposed attribute compression method shows better rate-distortion (R-D) performance than the latest MPEG G-PCC (both on reference software TMC13-v22.0 and GeS-TM-v2.0).&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bib5&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@article{yin2023,&#xA;author = {YIN Qian, ZHANG Xinfeng, HUANG Hongyue, WANG Shanshe, MA Siwei},&#xA;title = {Lossy Point Cloud Attribute Compression with Subnode-Based Prediction},&#xA;journal = {ZTE Communications},&#xA;volume = {21},&#xA;number = {4},&#xA;pages = {29-37},&#xA;year = {2023},&#xA;doi = {10.12142/ZTECOM.202304004}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Real-Time Scene-Aware LiDAR Point Cloud Compression Using Semantic Prior Representation&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Lili Zhao, Kai-Kuang Ma, Zhili Liu, &lt;strong&gt;Qian Yin&lt;/strong&gt; and Jianwen Chen&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;journal small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;vol. 32&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;no. 8&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 5623-5637&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2022&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/TCSVT.2022.3145513&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;abs6&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bib6&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;abs6&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; Existing LiDAR point cloud compression (PCC) methods tend to treat compression as a fidelity issue, without sufficiently addressing its machine perception aspect. The latter issue is often encountered by the decoder agents that might aim to conduct scene-understanding related tasks only, such as computing the localization information. For tackling this challenge, a novel LiDAR PCC system is proposed to compress the point cloud geometry, which contains a back channel for allowing the decoder to initiate such request to the encoder. The key success of our PCC method lies in our proposed semantic prior representation (SPR) and its lossy encoding algorithm with variable precision to generate the final bitstream; the entire process is fast and achieves real-time performance. Note that our SPR is a compact and effective representation of three-dimensional (3D) input point clouds, and it consists of labels, predictions, and residuals. These information can be generated by first exploiting a scene-aware object segmentation to a set of 2D range images (frames) individually, which were generated from the 3D point clouds via a projection process. Based on the generated labels, the pixels associated with those moving objects are considered as noisy information and should be removed for not only saving bit budget on transmission but also, most importantly, improving the accuracy of localization computed at the decoder. Experimental results conducted on the commonly-used test dataset have shown that our proposed system outperforms the MPEG‚Äôs G-PCC (TMC13-v14.0) in a large bitrate range. In fact, the performance gap will become even larger when more and/or large moving objects are involved in the input point clouds.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bib6&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@ARTICLE{9690112,&#xA;author={Zhao, Lili and Ma, Kai-Kuang and Liu, Zhili and Yin, Qian and Chen, Jianwen},&#xA;journal={IEEE Transactions on Circuits and Systems for Video Technology}, &#xA;title={Real-Time Scene-Aware LiDAR Point Cloud Compression Using Semantic Prior Representation}, &#xA;year={2022},&#xA;volume={32},&#xA;number={8},&#xA;pages={5623-5637},&#xA;doi={10.1109/TCSVT.2022.3145513}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;&lt;/ol&gt;&#xD;&#xA;&#xD;&#xA;&#xA;&lt;h1 id=&#34;conference&#34;&gt;Conference Publications&lt;/h1&gt;&#xD;&#xA;&lt;ol id=&#34;conference-list&#34;&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        A Fast Bit Allocation Refinement for Video Rate Control&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Longtao Fen, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Jiaqi Zhang, Lin Li, Qi Wang and Siwei Ma&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2025 Data Compression Conference (DCC) (Poster)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 1-1&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2025&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/DCC62719.2025.00054&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc1&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc1&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc1&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; Since the introduction of hierarchical picture prediction structure in the advanced video coding (AVC), the hierarchical coding structure (HCS) has been widely adopted and continuously improved in video coding standards. Correspondingly, the HCS-based bit allocation methods in rate control have also emerged endlessly. Considering that pictures in higher temporal levels (TLs) of HCS usually refer to pictures in lower TLs, most methods tend to allocate more bits to pictures in lower TLs. However, these methods do not fully consider the correlation of picture quality in different TLs, which leads to the bit allocation waste and the coding performance degradation. To address this issue, we propose a fast bit allocation refinement method that can adapt to different video rate control approaches. Fig. 1 shows the overall framework of the proposed method. In general, our method is to appropriately adjust the bit allocation of pictures in lower TLs according to the relationship between the quality of pictures in different TLs. Specifically, based on the hyperbolic rate-distortion (RD) model and initial allocated bits, the quality of picture in higher TLs is first predicted and then used to estimate the quality of picture in lower TLs. Subsequently, the bits of picture in lower TLs are derived using estimated quality and its RD model. Finally, the final allocated bits of picture in lower TLs are adjusted by comparing the estimated and initial allocated bits. Experimental results show that our method can improve the coding performance of different rate control methods without introducing latency and encoding complexity.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc1&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@INPROCEEDINGS{10992326,&#xA;  author={Feng, Longtao and Yin, Qian and Zhang, Jiaqi and Li, Lin and Wang, Qi and Ma, Siwei},&#xA;  booktitle={2025 Data Compression Conference (DCC)}, &#xA;  title={A Fast Bit Allocation Refinement for Video Rate Control}, &#xA;  year={2025},&#xA;  month={Mar.},&#xA;  volume={},&#xA;  number={},&#xA;  pages={366-366},&#xA;  doi={10.1109/DCC62719.2025.00054}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Compressed Domain Prior-Guided Video Super-Resolution for Cloud Gaming Content&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Qizhe Wang, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Zhimeng Huang, Weijia Jiang, Yi Su, Siwei Ma and Jiaqi Zhang&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2025 Data Compression Conference (DCC) (Oral)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 83‚Äì92&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2025&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/DCC62719.2025.00016&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc2&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc2&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc2&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; Cloud gaming is an advanced form of Internet service that necessitates local terminals to decode within limited resources and time latency. Super-Resolution (SR) techniques are often employed on these terminals as an efficient way to reduce the required bit-rate bandwidth for cloud gaming. However, insufficient attention has been paid to SR of compressed game video content. Most SR networks amplify block artifacts and ringing effects in decoded frames while ignoring edge details of game content, leading to unsatisfactory reconstruction results. In this paper, we propose a novel lightweight network called Coding Prior-Guided Super-Resolution (CPGSR) to address the SR challenges in compressed game video content. First, we design a Compressed Domain Guided Block (CDGB) to extract features of different depths from coding priors, which are subsequently integrated with features from the U-net backbone. Then, a series of re-parameterization blocks are utilized for reconstruction. Ultimately, inspired by the quantization in video coding, we propose a partitioned focal frequency loss to effectively guide the model&amp;rsquo;s focus on preserving high-frequency information. Extensive experiments demonstrate the advancement of our approach.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc2&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@INPROCEEDINGS{10992520,&#xA;  author={Wang, Qizhe and Yin, Qian and Huang, Zhimeng and Jiang, Weijia and Su, Yi and Ma, Siwei and Zhang, Jiaqi},&#xA;  booktitle={2025 Data Compression Conference (DCC)}, &#xA;  title={Compressed Domain Prior-Guided Video Super-Resolution for Cloud Gaming Content}, &#xA;  year={2025},&#xA;  pages={83-92},&#xA;  doi={10.1109/DCC62719.2025.00016}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Adaptive Block-Level Quality Parameter Adjustment Towards Low Video Bit-Rate Fluctuation&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Longtao Feng, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Huiwen Ren, Zhao Wang, Siwei Ma and Yuwen He&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2024 IEEE International Conference on Visual Communications and Image Processing (VCIP)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 1‚Äì5&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2024&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/VCIP63160.2024.10849847&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc3&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc3&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc3&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; Existing quantization parameter (QP) adjustment methods in video coding often focus solely on coding efficiency and ignore the impact of bit-rate fluctuations on video transmission and bandwidth waste. This is mainly because intra pictures, in a hierarchical coding structure, are allocated smaller QP and thus consume more bits. To address this issue, we propose an adaptive block-level QP adjustment method. Specifically, intra picture importance (IPI) is first introduced to evaluate the adjustability of intra picture QP. For intra pictures whose QP can be adjusted, we further propose block importance (BI) to determine their optimal block-level QP adjustment. Experimental results show that our proposed method reduce the bit-rate fluctuations while basically maintaining the coding performance. Notably, significant improvements can be observed in high-resolution videos, with a reduction of approximately 11% in bit-rate fluctuations.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc3&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@INPROCEEDINGS{10849847,&#xA;  author={Feng, Longtao and Yin, Qian and Ren, Huiwen and Wang, Zhao and Ma, Siwei and He, Yuwen},&#xA;  booktitle={2024 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, &#xA;  title={Adaptive Block-Level Quality Parameter Adjustment Towards Low Video Bit-Rate Fluctuation}, &#xA;  year={2024},&#xA;  pages={1-5},&#xA;  doi={10.1109/VCIP63160.2024.10849847}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Towards Robust Visual Localization Using Multi-View Images and HD Vector Map&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Lili Zhao, Zhili Liu, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Lei Yang, Meng Guo&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2024 IEEE International Conference on Image Processing (ICIP)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 814‚Äì820&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2024&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/ICIP51287.2024.10647615&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc4&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc4&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc4&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; Robust and accurate localization is highly desired in intelligent driving and robotic navigation. Existing methods highly rely on feature maps and complex parameter tuning, while suffering from ineffective data association, heavy computation, high dependency on training data and low robustness. In this paper, we propose a high-robust and cost-effective visual localization system, which jointly exploits the semantic information of Bird‚Äôs-Eye-View (BEV) representation from multi-view images and the vectorized High Definition (HD) map. We formulate the visual localization as cross-modal data association issue and innovatively project the vectorized landmarks of HD map into BEV semantic map. Finally, the highly accurate vehicle‚Äôs pose can be estimated by pose optimization based on direct image alignment. Extensive simulations experimented on nuScenes dataset show that the proposed method can deliver robust and accurate localization results under various scenarios. In addition, the proposed system is convenient for large-scale deployment and has been tested on the commercial test car.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc4&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@INPROCEEDINGS{10647615,&#xA;  author={Zhao, Lili and Liu, Zhili and Yin, Qian and Yang, Lei and Guo, Meng},&#xA;  booktitle={2024 IEEE International Conference on Image Processing (ICIP)},&#xA;  title={Towards Robust Visual Localization Using Multi-View Images and HD Vector Map},&#xA;  year={2024},&#xA;  pages={814-820},&#xA;  doi={10.1109/ICIP51287.2024.10647615}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        A Dynamic Point Cloud Dataset for MPEG Point Cloud Compression and Performance Analysis&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Lili Zhao, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Lancao Ren, Lei Yang, Chuanmin Jia, Siwei Ma&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2024 Data Compression Conference (DCC)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 1‚Äì1&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2024&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/DCC58796.2024.00121&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc5&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc5&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc5&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; Recent years witnessed the development in MPEG point cloud compression (PCC). However, the exploration of inter-frame coding may be impeded due to the lack of dynamic point clouds (point cloud sequences). To promote the development of PCC technology, we propose Dynamic3D, a dynamic 3D point cloud dataset with high-quality real-captured 3D persons and objects. There are several appealing properties: 1) Dynamic scenes: It contains five sequences and each sequence comprises 600 frames with temporal variation; 2) Complex content: instead of a single person or object in the existing dataset from MPEG, our established dataset contains multiple persons or both person and objects; 3) Realistic capture: the color industrial cameras and infrared cameras are used for data acquisition. This dataset provides the vast exploration space for PCC, especially the elimination of temporal redundancy. Extensive simulations are conducted on this dataset by using the reference software of MPEG G-PCC and V-PCC, i.e., (GeS-TM and TMC2), delivering observations, analysis and opportunities for the future research of PCC.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc5&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@INPROCEEDINGS{10533804,&#xA;  author={Zhao, Lili and Yin, Qian and Ren, Lancao and Yang, Lei and Jia, Chuanmin and Ma, Siwei},&#xA;  booktitle={2024 Data Compression Conference (DCC)},&#xA;  title={A Dynamic Point Cloud Dataset for MPEG Point Cloud Compression and Performance Analysis},&#xA;  year={2024},&#xA;  pages={1-1},&#xA;  doi={10.1109/DCC58796.2024.00121}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Learning Spatial-Temporal Embeddings for Sequential Point Cloud Frame Interpolation&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Lili Zhao, Zhuoqun Sun, Lancao Ren, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Lei Yang, Meng Guo&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2023 IEEE International Conference on Image Processing (ICIP)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 810‚Äì814&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2023&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/ICIP49359.2023.10221958&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc6&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc6&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc6&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; A point cloud sequence is usually acquired at a low frame rate owing to the limitations from the sensing equipment. Consequently, the immersive experience of the virtual reality might be greatly degraded. To tackle this issue, a point cloud frame interpolation process can be used to increase the frame rate of the acquired point cloud sequence by generating new frames between the consecutive ones. However, it is still challenging for deep neural networks to synthesize high-fidelity point clouds, especially for those with complex geometric details and large motion. In this paper, a novel frame interpolation network is proposed, which jointly exploits the spatial features and flows. The key success of our method lies in the developed spatial-temporal feature propagation module and temporal-aware feature-to-point mapping module. The former effectively embeds the spatial features and scene flows into a spatial-temporal feature representation (STFR). The latter generates a much improved target frame from STFR. Extensive experimental results have demonstrated that our method has achieved the best performance in most cases.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc6&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@INPROCEEDINGS{10221958,&#xA;  author={Zhao, Lili and Sun, Zhuoqun and Ren, Lancao and Yin, Qian and Yang, Lei and Guo, Meng},&#xA;  booktitle={2023 IEEE International Conference on Image Processing (ICIP)},&#xA;  title={Learning Spatial-Temporal Embeddings for Sequential Point Cloud Frame Interpolation},&#xA;  year={2023},&#xA;  pages={810-814},&#xA;  doi={10.1109/ICIP49359.2023.10221958}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Model-Driven Compression for Digital Human Using Multi-Granularity Representations&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Ruoke Yan, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Xinfeng Zhang, Siwei Ma&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2023 IEEE International Conference on Multimedia and Expo (ICME)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 690‚Äì695&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2023&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/ICME55011.2023.00124&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc7&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc7&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc7&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; With the popularity of the &amp;ldquo;metaverse&amp;rdquo;, the applications of virtual digital humans are emerging in entertainment and communication fields, etc, where effective digital human compression schemes are urgently needed to process huge amounts of data. Most of the existing coding methods focus on reducing spatial redundancy in terms of the signal level but ignore the consideration of visual perception. In this paper, we propose a novel digital human model-driven compression framework by using multi-granularity representations. A coarse-grained layer based on the Skinned Multi-Person Linear model (SMPL) is introduced to extract general structures, while the normal images are used to represent fine-grained details via pose consistency-based predictions. Then, the SMPL parameters and normal images are encoded to achieve high-quality compression at extremely low bitrates. Experimental results demonstrate that the proposed method provides better coding performance with superior perceptual quality compared to the state-of-the-art 3D model compression methods.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc7&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@INPROCEEDINGS{10219876,&#xA;  author={Yan, Ruoke and Yin, Qian and Zhang, Xinfeng and Ma, Siwei},&#xA;  booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)},&#xA;  title={Model-Driven Compression for Digital Human Using Multi-Granularity Representations},&#xA;  year={2023},&#xA;  pages={690-695},&#xA;  doi={10.1109/ICME55011.2023.00124}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Multi-Scale end-to-End Learning for Point Cloud Geometry Compression&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Yiqun Xu, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Shanshe Wang, Xinfeng Zhang, Siwei Ma, Wen Gao&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2022 IEEE International Conference on Image Processing (ICIP)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 2107‚Äì2111&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2022&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/ICIP46576.2022.9898058&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc8&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc8&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc8&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&lt;strong&gt;Abstract&lt;/strong&gt; As 3D scanning devices and depth sensors advance, point clouds have attracted increasing attention as a format for 3D representation. Nevertheless, the tremendous amount of data in point clouds significantly burden transmission and storage. To address these problems, we propose a multi-scale end-to-end framework for point cloud geometry compression. Firstly, point transformer is used to extract the global feature of geometry information, embedding the geometry information and the relation among points. Secondly, the multi-scale neighbor embedding strategy is used to extract the level of details within multi-scale and multi-resolution feature of point clouds. Finally, to reconstruct the point cloud with high quality in the decoder, the local spatial information is restored via graph spatial extension based on local down-sampled features and global features. Experimental results show that we achieve around 34% bit rate reduction on average over competitive point cloud geometry compression methods.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc8&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;@INPROCEEDINGS{9898058,&#xA;  author={Xu, Yiqun and Yin, Qian and Wang, Shanshe and Zhang, Xinfeng and Ma, Siwei and Gao, Wen},&#xA;  booktitle={2022 IEEE International Conference on Image Processing (ICIP)},&#xA;  title={Multi-Scale end-to-End Learning for Point Cloud Geometry Compression},&#xA;  year={2022},&#xA;  pages={2107-2111},&#xA;  doi={10.1109/ICIP46576.2022.9898058}&#xA;}&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Foldingnet-Based Geometry Compression of Point Cloud with Multi Descriptions&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Xiaoqi Ma, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Xinfeng Zhang, Lv Tang&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2022 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 1‚Äì6&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2022&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/ICMEW56448.2022.9859339&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc9&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc9&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc9&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;Traditional point cloud compression (PCC) methods are not effective at extremely low bit rate scenarios because of the uniform quantization. Although learning-based PCC approaches can achieve superior compression performance, they need to train multiple models for different bit rate, which greatly increases the training complexity and memory storage. To tackle these challenges, a novel FoldingNet-based Point Cloud Geometry Compression (FN-PCGC) framework is proposed in this paper. Firstly, the point cloud is divided into several descriptions by a Multiple-Description Generation (MDG) module. Then a point-based Auto-Encoder with the Multi-scale Feature Extraction (MFE) is introduced to compress all the descriptions. Experimental results show that the proposed method outperforms the MPEG G-PCC and Draco with about 30% ~ 80% gain on average.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc9&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        RAI-Net: Range-Adaptive LiDAR Point Cloud Frame Interpolation Network&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Lili Zhao, Zezhi Zhu, Xuhu Lin, Xuezhou Guo, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Wenyi Wang, Jianwen Chen&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2021 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 1‚Äì6&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2021&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/BMSB53066.2021.9547131&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc10&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc10&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc10&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;LiDAR point cloud frame interpolation, which synthesizes the intermediate frame between the captured frames, has emerged as an important issue for many applications. Especially for reducing the amounts of point cloud transmission, it is by predicting the intermediate frame based on the reference frames to upsample data to high frame rate ones. However, due to high-dimensional and sparse characteristics of point clouds, it is more difficult to predict the intermediate frame for LiDAR point clouds than videos. In this paper, we propose a novel LiDAR point cloud frame interpolation method, which exploits range images (RIs) as an intermediate representation with CNNs to conduct the frame interpolation process. Considering the inherited characteristics of RIs differ from that of color images, we introduce spatially adaptive convolutions to extract range features adaptively, while a high-efficient flow estimation method is presented to generate optical flows. The proposed model then warps the input frames and range features, based on the optical flows to synthesize the interpolated frame. Extensive experiments on the KITTI dataset have clearly demonstrated that our method consistently achieves superior frame interpolation results with better perceptual quality to that of using state-of-the-art video frame interpolation methods. The proposed method could be integrated into any LiDAR point cloud compression systems for inter prediction.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc10&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        Lossless Point Cloud Attribute Compression with Normal-based Intra Prediction&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        &lt;strong&gt;Qian Yin&lt;/strong&gt;, Qingshan Ren, Lili Zhao, Wenyi Wang, Jianwen Chen&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2021 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 1‚Äì5&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2021&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/BMSB53066.2021.9547021&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc11&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc11&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc11&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;The sparse LiDAR point clouds become more and more popular in various applications, e.g., the autonomous driving. However, for this type of data, there exists much under-explored space in the corresponding compression framework proposed by MPEG, i.e., geometry-based point cloud compression (G-PCC). In G-PCC, only the distance-based similarity is considered in the intra prediction for the attribute compression. In this paper, we propose a normal-based intra prediction scheme, which provides a more efficient lossless attribute compression by introducing the normals of point clouds. The angle between normals is used to further explore accurate local similarity, which optimizes the selection of predictors. We implement our method into the G-PCC reference software. Experimental results over LiDAR acquired datasets demonstrate that our proposed method is able to deliver better compression performance than the G-PCC anchor, with 2.1% gains on average for lossless attribute coding.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc11&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;   &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        3D Facial Expression Recognition Using Deep Feature Fusion CNN&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Kun Tian, Liaoyuan Zeng, Sean McGrath, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Wenyi Wang&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;conference small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;2019 30th Irish Signals and Systems Conference (ISSC)&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;pp. 1‚Äì6&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2019&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;links small-text&#34;&gt;&#xD;&#xA;        &lt;a href=&#34;https://doi.org/10.1109/ISSC.2019.8904930&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;absc12&#39;); return false;&#34;&gt;Abstract&lt;/a&gt; |&#xD;&#xA;        &lt;a href=&#34;#&#34; onclick=&#34;toggleContent(&#39;bibc12&#39;); return false;&#34;&gt;BibTeX&lt;/a&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div id=&#34;absc12&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;As an important way of human communication, facial expression not only reflects our mental activities but also provides useful information for human behavior research. Recently, 3D technology becomes promising method to achieve robust facial expression analysis. 3D face scans are robust to lighting and pose variations. In this paper, a novel deep feature fusion convolution neural network (CNN) is designed for 3D facial expression recognition (FER). Each 3D face scan is firstly represented as 2D facial attribute maps (including depth, normal, and shape index values). Then, we combine different of facial attribute maps to learn facial representations by fine-tuning a pre-trained deep feature fusion CNN subnet trained from a large-scale image dataset for universal visual tasks. Moreover, Global Average Pooling is utilized to reduce the number of parameters to decrease the effect of overfitting. The experiments are conducted on the Bosphorus database and the results demonstrate the effectiveness of the proposed method.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;pre id=&#34;bibc12&#34; class=&#34;hidden-content&#34;&gt;&#xD;&#xA;&#xD;&#xA;      &lt;/pre&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;patents&#34;&gt;Patents&lt;/h1&gt;&#xD;&#xA;&lt;ul id=&#34;patent-list&#34;&gt;&#xD;&#xA;  &#xD;&#xA;    &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        ‰∏ÄÁßçÂü∫‰∫éLEDÁöÑÊ≤âÊµ∏ÂºèËôöÊãüÁéØÂ¢ÉÁ≥ªÁªüÂíåÊñπÊ≥ï&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        &lt;strong&gt;Qian Yin&lt;/strong&gt;, Haopeng Lu, Mingjia Yang, Ruiqi Li, Yizong Wang, Xinfeng Zhang, Shanshe Wang and Siwei Ma&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;patent small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;CN202510658690.8&lt;/em&gt;, &lt;em&gt;Áî≥ËØ∑&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt; May.&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2025&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;    &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        ‰∏ÄÁßçÂü∫‰∫éÂπ≥ÊªëÊª§Ê≥¢ÁöÑÁÇπ‰∫ëÊï∞ÊçÆÂ§ÑÁêÜÊñπÊ≥ï„ÄÅË£ÖÁΩÆÂèäËÆæÂ§á&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        &lt;strong&gt;Qian Yin&lt;/strong&gt;, Xinfeng ZhangÔºå Siwei Ma, Liuxin Zhang and Qianying Wang&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;patent small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;CN202310884232.7&lt;/em&gt;, &lt;em&gt;Áî≥ËØ∑&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt; Jul.&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2023&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;    &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        ‰∏âÁª¥Áúü‰∫∫Ê®°ÂûãÂ§ÑÁêÜÊñπÊ≥ï„ÄÅÂ§ÑÁêÜË£ÖÁΩÆ„ÄÅÁîµÂ≠êËÆæÂ§áÂèäÂ≠òÂÇ®‰ªãË¥®&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        Siwei Ma, Ruoke Yan, &lt;strong&gt;Qian Yin&lt;/strong&gt;, Xinfeng Zhang, Jianhui Chang and Shanshe Wang&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;patent small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;CN202310416460.1&lt;/em&gt;, &lt;em&gt;ÊéàÊùÉ&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt; Sep.&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2023&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;    &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        ÁÇπ‰∫ëÁÜµÁºñÁ†ÅÊñπÊ≥ï,Ëß£Á†ÅÊñπÊ≥ï,Ë£ÖÁΩÆ,ËÆæÂ§áÂèäÂèØËØªÂ≠òÂÇ®‰ªãË¥®&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        &lt;strong&gt;Qian Yin&lt;/strong&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;patent small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;CN202111467830.1&lt;/em&gt;, &lt;em&gt;ÂÖ¨ÂºÄ&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt; Jun.&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2023&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;    &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        ‰∏ÄÁßçÁÇπ‰∫ëÊï∞ÊçÆÂ§ÑÁêÜÊñπÊ≥ï„ÄÅË£ÖÁΩÆÂèäËÆæÂ§á&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        &lt;strong&gt;Qian Yin&lt;/strong&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;patent small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;CN202111468170.9&lt;/em&gt;, &lt;em&gt;ÂÖ¨ÂºÄ&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt; Jun.&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2023&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;    &lt;li&gt;&#xD;&#xA;      &lt;div class=&#34;title&#34; style=&#34;font-family: &#39;Arial Rounded MT Bold&#39;; font-weight: bold;&#34;&gt;&#xD;&#xA;        ‰∏ÄÁßçÁÇπ‰∫ëÂ§ÑÁêÜÊñπÊ≥ï„ÄÅË£ÖÁΩÆÂèäÁîµÂ≠êËÆæÂ§á&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;authors small-text&#34;&gt;&#xD;&#xA;        &lt;strong&gt;Qian Yin&lt;/strong&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;patent small-text green-text&#34;&gt;&#xD;&#xA;        &lt;em&gt;CN202111475199.X&lt;/em&gt;, &lt;em&gt;ÂÖ¨ÂºÄ&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt; Jun.&lt;/em&gt;&#xD;&#xA;        , &lt;em&gt;2023&lt;/em&gt;.&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;    &lt;/li&gt;&#xD;&#xA;  &#xD;&#xA;&lt;/ul&gt;&#xA;&lt;/section&gt;</description>
    </item>
  </channel>
</rss>
